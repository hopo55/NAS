{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSearchSpace(object):\n",
    "\n",
    "    def __init__(self, target_classes):\n",
    "\n",
    "        self.target_classes = target_classes\n",
    "        self.vocab = self.vocab_dict()\n",
    "\n",
    "\n",
    "    def vocab_dict(self):\n",
    "    \t# define the allowed nodes and activation functions\n",
    "        nodes = [8, 16, 32, 64, 128, 256, 512]\n",
    "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "        \n",
    "        # initialize lists for keys and values of the vocabulary\n",
    "        layer_params = []\n",
    "        layer_id = []\n",
    "        \n",
    "        # for all activation functions for each node\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(act_funcs)):\n",
    "            \t\n",
    "                # create an id and a configuration tuple (node, activation)\n",
    "                layer_params.append((nodes[i], act_funcs[j]))\n",
    "                layer_id.append(len(act_funcs) * i + j + 1)\n",
    "        \n",
    "        # zip the id and configurations into a dictionary\n",
    "        vocab = dict(zip(layer_id, layer_params))\n",
    "        \n",
    "        # add dropout in the volcabulary\n",
    "        vocab[len(vocab) + 1] = (('dropout'))\n",
    "        \n",
    "        # add the final softmax/sigmoid layer in the vocabulary\n",
    "        if self.target_classes == 2:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
    "        else:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
    "        return vocab\n",
    "\n",
    "\n",
    "\t# function to encode a sequence of configuration tuples\n",
    "    def encode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        encoded_sequence = []\n",
    "        for value in sequence:\n",
    "            encoded_sequence.append(keys[values.index(value)])\n",
    "        return encoded_sequence\n",
    "\n",
    "\n",
    "\t# function to decode a sequence back to configuration tuples\n",
    "    def decode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        decoded_sequence = []\n",
    "        for key in sequence:\n",
    "            decoded_sequence.append(values[keys.index(key)])\n",
    "        return decoded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (8, 'sigmoid')\n",
      "2 (8, 'tanh')\n",
      "3 (8, 'relu')\n",
      "4 (8, 'elu')\n",
      "5 (16, 'sigmoid')\n",
      "6 (16, 'tanh')\n",
      "7 (16, 'relu')\n",
      "8 (16, 'elu')\n",
      "9 (32, 'sigmoid')\n",
      "10 (32, 'tanh')\n",
      "11 (32, 'relu')\n",
      "12 (32, 'elu')\n",
      "13 (64, 'sigmoid')\n",
      "14 (64, 'tanh')\n",
      "15 (64, 'relu')\n",
      "16 (64, 'elu')\n",
      "17 (128, 'sigmoid')\n",
      "18 (128, 'tanh')\n",
      "19 (128, 'relu')\n",
      "20 (128, 'elu')\n",
      "21 (256, 'sigmoid')\n",
      "22 (256, 'tanh')\n",
      "23 (256, 'relu')\n",
      "24 (256, 'elu')\n",
      "25 (512, 'sigmoid')\n",
      "26 (512, 'tanh')\n",
      "27 (512, 'relu')\n",
      "28 (512, 'elu')\n",
      "29 dropout\n",
      "30 (10, 'softmax')\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPSearchSpace(10)\n",
    "\n",
    "x = mlp.vocab_dict()\n",
    "for key, value in x.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   NAS PARAMETERS                     #\n",
    "########################################################\n",
    "CONTROLLER_SAMPLING_EPOCHS = 10\n",
    "SAMPLES_PER_CONTROLLER_EPOCH = 10\n",
    "CONTROLLER_TRAINING_EPOCHS = 10\n",
    "ARCHITECTURE_TRAINING_EPOCHS = 10\n",
    "CONTROLLER_LOSS_ALPHA = 0.9\n",
    "\n",
    "########################################################\n",
    "#               CONTROLLER PARAMETERS                  #\n",
    "########################################################\n",
    "CONTROLLER_LSTM_DIM = 100\n",
    "CONTROLLER_OPTIMIZER = 'Adam'\n",
    "CONTROLLER_LEARNING_RATE = 0.01\n",
    "CONTROLLER_DECAY = 0.1\n",
    "CONTROLLER_MOMENTUM = 0.0\n",
    "CONTROLLER_USE_PREDICTOR = True\n",
    "\n",
    "########################################################\n",
    "#                   MLP PARAMETERS                     #\n",
    "########################################################\n",
    "MAX_ARCHITECTURE_LENGTH = 3\n",
    "MLP_OPTIMIZER = 'Adam'\n",
    "MLP_LEARNING_RATE = 0.01\n",
    "MLP_DECAY = 0.0\n",
    "MLP_MOMENTUM = 0.0\n",
    "MLP_DROPOUT = 0.2\n",
    "MLP_LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "MLP_ONE_SHOT = True\n",
    "\n",
    "########################################################\n",
    "#                   DATA PARAMETERS                    #\n",
    "########################################################\n",
    "TARGET_CLASSES = 3\n",
    "\n",
    "########################################################\n",
    "#                  OUTPUT PARAMETERS                   #\n",
    "########################################################\n",
    "TOP_N = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hopo5\\Anaconda3\\envs\\home\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\hopo5\\Anaconda3\\envs\\home\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\Users\\hopo5\\Anaconda3\\envs\\home\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim as optimizers\n",
    "from torch.nn import Sequential, Linear, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(MLPSearchSpace):\n",
    "    def __init__(self):\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.mlp_optimizer = MLP_OPTIMIZER\n",
    "        self.mlp_lr = MLP_LEARNING_RATE\n",
    "        self.mlp_decay = MLP_DECAY\n",
    "        self.mlp_momentum = MLP_MOMENTUM\n",
    "        self.mlp_dropout = MLP_DROPOUT\n",
    "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
    "        self.mlp_one_shot = MLP_ONE_SHOT\n",
    "        self.metrics = ['accuracy']\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        if self.mlp_one_shot:\n",
    "            self.weights_file = 'LOGS/shared_weights.pkl'\n",
    "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
    "            if not os.path.exists(self.weights_file):\n",
    "                print(\"Initializing shared weights dictionary...\")\n",
    "                self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def create_model(self, sequence, mlp_input_shape):\n",
    "        layer_configs = self.decode_sequence(sequence)\n",
    "        model = Sequential()\n",
    "\n",
    "        if len(mlp_input_shape) > 1:\n",
    "            model.add_module('flatten', Flatten(mlp_input_shape))\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if layer_conf == 'dropout':\n",
    "                    model.add_module('dropout', Dropout(self.mlp_dropout))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        else:\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if i == 0:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1], input_shape=mlp_input_shape))\n",
    "                elif layer_conf == 'dropout':\n",
    "                    model.add_module(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        if self.mlp_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.mlp_optimizer)(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay)\n",
    "\n",
    "        return optim\n",
    "\n",
    "    def update_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) == 0:\n",
    "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j], 'weights': layer.get_weights()}, ignore_index=True)\n",
    "                else:\n",
    "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def set_model_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                \n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) > 0:\n",
    "                    print(\"Transferring weights for layer:\", config_ids[j])\n",
    "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
    "                \n",
    "                j += 1\n",
    "\n",
    "    def train_model(self, model, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
    "        if self.mlp_one_shot:\n",
    "            self.set_model_weights(model)\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "            self.update_weights(model)\n",
    "        else:\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# generator = MLPGenerator()\n",
    "# model = generator.create_model(sequence, np.shape(x[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LSTM, Linear\n",
    "\n",
    "# from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(MLPSearchSpace):\n",
    "\tdef __init__(self):\n",
    "\t\tself.max_len = MAX_ARCHITECTURE_LENGTH\n",
    "\t\tself.controller_lstm_dim = CONTROLLER_LSTM_DIM\n",
    "\t\tself.controller_optimizer = CONTROLLER_OPTIMIZER\n",
    "\t\tself.controller_lr = CONTROLLER_LEARNING_RATE\n",
    "\t\tself.controller_decay = CONTROLLER_DECAY\n",
    "\t\tself.controller_momentum = CONTROLLER_MOMENTUM\n",
    "\t\tself.use_predictor = CONTROLLER_USE_PREDICTOR\n",
    "\n",
    "\t\tself.controller_weights = 'LOGS/controller_weights.h5'\n",
    "\t\tself.seq_data = []\n",
    "\n",
    "\t\tsuper().__init__(TARGET_CLASSES)\n",
    "\n",
    "\t\tself.controller_classes = len(self.vocab) + 1\n",
    "        \n",
    "\t# Controller Architecture\n",
    "\tdef control_model(self, controller_input_shape, controller_batch_size):\n",
    "\t\tmain_input = torch.tensor(controller_batch_size, controller_input_shape) # name='main_input'\n",
    "\t\tx = LSTM(self.controller_lstm_dim)(main_input) # return_sequences=True\n",
    "\t\tmain_output = Linear(self.controller_classes)(x) # activation='softmax', name='main_output'\n",
    "\t\tmodel = Model(inputs=[main_input], outputs=[main_output]) #???????????\n",
    "\t\t\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef hybrid_control_model(self, controller_input_shape, controller_batch_size):\n",
    "\t\tmain_input = torch.tensor(controller_batch_size, controller_input_shape) # name='main_input'\n",
    "\t\tx = LSTM(self.controller_lstm_dim)(main_input) # return_sequences=True\n",
    "\t\tpredictor_output = Linear(1)(x) # , activation='sigmoid', name='predictor_output'\n",
    "\t\tmain_output = Linear(self.controller_classes)(x) # , activation='softmax', name='main_output'\n",
    "\t\tmodel = Model(inputs=[main_input], outputs=[main_output, predictor_output])\n",
    "\t\t\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef train_control_model(self, model, x_data, y_data, loss_func, controller_batch_size, nb_epochs):\n",
    "\t\tif self.controller_optimizer == 'sgd':\n",
    "\t\t\toptim = optimizers.SGD(lr=self.controller_lr, decay=self.controller_decay, momentum=self.controller_momentum, clipnorm=1.0)\n",
    "\t\telse:\n",
    "\t\t\toptim = getattr(optimizers, self.controller_optimizer)(lr=self.controller_lr, decay=self.controller_decay, clipnorm=1.0)\n",
    "\t\tmodel.compile(optimizer=optim, loss={'main_output': loss_func})\n",
    "\t\tif os.path.exists(self.controller_weights):\n",
    "\t\t\tmodel.load_weights(self.controller_weights)\n",
    "\t\tprint(\"TRAINING CONTROLLER...\")\n",
    "\t\tmodel.fit({'main_input': x_data},\n",
    "\t\t\t\t\t{'main_output': y_data.reshape(len(y_data), 1, self.controller_classes)},\n",
    "\t\t\t\t\tepochs=nb_epochs,\n",
    "\t\t\t\t\tbatch_size=controller_batch_size,\n",
    "\t\t\t\t\tverbose=0)\n",
    "\t\tmodel.save_weights(self.controller_weights)\n",
    "\n",
    "\tdef sample_architecture_sequences(self, model, number_of_samples):\n",
    "\t\tfinal_layer_id = len(self.vocab)\n",
    "\t\tdropout_id = final_layer_id - 1\n",
    "\t\tvocab_idx = [0] + list(self.vocab.keys())\n",
    "\t\tsamples = []\n",
    "\t\tprint(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
    "\t\tprint('------------------------------------------------------')\n",
    "\t\twhile len(samples) < number_of_samples:\n",
    "\t\t\tseed = []\n",
    "\t\t\twhile len(seed) < self.max_len:\n",
    "\t\t\t\tsequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
    "\t\t\t\tsequence = sequence.reshape(1, 1, self.max_len - 1)\n",
    "\t\t\t\tif self.use_predictor:\n",
    "\t\t\t\t\t(probab, _) = model.predict(sequence)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprobab = model.predict(sequence)\n",
    "\t\t\t\tprobab = probab[0][0]\n",
    "\t\t\t\tnext = np.random.choice(vocab_idx, size=1, p=probab)[0]\n",
    "\t\t\t\tif next == dropout_id and len(seed) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif next == final_layer_id and len(seed) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif next == final_layer_id:\n",
    "\t\t\t\t\tseed.append(next)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif len(seed) == self.max_len - 1:\n",
    "\t\t\t\t\tseed.append(final_layer_id)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif not next == 0:\n",
    "\t\t\t\t\tseed.append(next)\n",
    "\t\t\tif seed not in self.seq_data:\n",
    "\t\t\t\tsamples.append(seed)\n",
    "\t\t\t\tself.seq_data.append(seed)\n",
    "\t\treturn samples\n",
    "\t\n",
    "\tdef get_predicted_accuracies_hybrid_model(self, model, seqs):\n",
    "\t\tpred_accuracies = []\n",
    "\t\tfor seq in seqs:\n",
    "\t\t\tcontrol_sequences = pad_sequences([seq], maxlen=self.max_len, padding='post')\n",
    "\t\t\txc = control_sequences[:, :-1].reshape(len(control_sequences), 1, self.max_len - 1)\n",
    "\t\t\t(_, pred_accuracy) = [x[0][0] for x in model.predict(xc)]\n",
    "\t\t\tpred_accuracies.append(pred_accuracy[0])\n",
    "\t\treturn pred_accuracies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
