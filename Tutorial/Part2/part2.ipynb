{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Space\n",
    "## Part-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSearchSpace(object):\n",
    "\n",
    "    def __init__(self, target_classes):\n",
    "\n",
    "        self.target_classes = target_classes\n",
    "        self.vocab = self.vocab_dict()\n",
    "\n",
    "    def vocab_dict(self):\n",
    "        nodes = [8, 16, 32, 64, 128, 256, 512]\n",
    "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "        layer_params = []\n",
    "        layer_id = []\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(act_funcs)):\n",
    "                layer_params.append((nodes[i], act_funcs[j]))\n",
    "                layer_id.append(len(act_funcs) * i + j + 1)\n",
    "        vocab = dict(zip(layer_id, layer_params))\n",
    "        vocab[len(vocab) + 1] = (('dropout'))\n",
    "        if self.target_classes == 2:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
    "        else:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
    "            \n",
    "        return vocab\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        encoded_sequence = []\n",
    "        for value in sequence:\n",
    "            encoded_sequence.append(keys[values.index(value)])\n",
    "\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        decoded_sequence = []\n",
    "        for key in sequence:\n",
    "            decoded_sequence.append(values[keys.index(key)])\n",
    "\n",
    "        return decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (8, 'sigmoid')\n",
      "2 (8, 'tanh')\n",
      "3 (8, 'relu')\n",
      "4 (8, 'elu')\n",
      "5 (16, 'sigmoid')\n",
      "6 (16, 'tanh')\n",
      "7 (16, 'relu')\n",
      "8 (16, 'elu')\n",
      "9 (32, 'sigmoid')\n",
      "10 (32, 'tanh')\n",
      "11 (32, 'relu')\n",
      "12 (32, 'elu')\n",
      "13 (64, 'sigmoid')\n",
      "14 (64, 'tanh')\n",
      "15 (64, 'relu')\n",
      "16 (64, 'elu')\n",
      "17 (128, 'sigmoid')\n",
      "18 (128, 'tanh')\n",
      "19 (128, 'relu')\n",
      "20 (128, 'elu')\n",
      "21 (256, 'sigmoid')\n",
      "22 (256, 'tanh')\n",
      "23 (256, 'relu')\n",
      "24 (256, 'elu')\n",
      "25 (512, 'sigmoid')\n",
      "26 (512, 'tanh')\n",
      "27 (512, 'relu')\n",
      "28 (512, 'elu')\n",
      "29 dropout\n",
      "30 (10, 'softmax')\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPSearchSpace(10)\n",
    "\n",
    "x = mlp.vocab_dict()\n",
    "for key, value in x.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   NAS PARAMETERS                     #\n",
    "########################################################\n",
    "CONTROLLER_SAMPLING_EPOCHS = 10\n",
    "SAMPLES_PER_CONTROLLER_EPOCH = 10\n",
    "CONTROLLER_TRAINING_EPOCHS = 10\n",
    "ARCHITECTURE_TRAINING_EPOCHS = 10\n",
    "CONTROLLER_LOSS_ALPHA = 0.9\n",
    "\n",
    "########################################################\n",
    "#               CONTROLLER PARAMETERS                  #\n",
    "########################################################\n",
    "CONTROLLER_LSTM_DIM = 100\n",
    "CONTROLLER_OPTIMIZER = 'Adam'\n",
    "CONTROLLER_LEARNING_RATE = 0.01\n",
    "CONTROLLER_DECAY = 0.1\n",
    "CONTROLLER_MOMENTUM = 0.0\n",
    "CONTROLLER_USE_PREDICTOR = True\n",
    "\n",
    "########################################################\n",
    "#                   MLP PARAMETERS                     #\n",
    "########################################################\n",
    "MAX_ARCHITECTURE_LENGTH = 3\n",
    "MLP_OPTIMIZER = 'Adam'\n",
    "MLP_LEARNING_RATE = 0.01\n",
    "MLP_DECAY = 0.0\n",
    "MLP_MOMENTUM = 0.0\n",
    "MLP_DROPOUT = 0.2\n",
    "MLP_LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "MLP_ONE_SHOT = True\n",
    "\n",
    "########################################################\n",
    "#                   DATA PARAMETERS                    #\n",
    "########################################################\n",
    "TARGET_CLASSES = 3\n",
    "\n",
    "########################################################\n",
    "#                  OUTPUT PARAMETERS                   #\n",
    "########################################################\n",
    "TOP_N = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim as optimizers\n",
    "from torch.nn import Sequential, Linear, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(MLPSearchSpace):\n",
    "    def __init__(self):\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.mlp_optimizer = MLP_OPTIMIZER\n",
    "        self.mlp_lr = MLP_LEARNING_RATE\n",
    "        self.mlp_decay = MLP_DECAY\n",
    "        self.mlp_momentum = MLP_MOMENTUM\n",
    "        self.mlp_dropout = MLP_DROPOUT\n",
    "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
    "        self.mlp_one_shot = MLP_ONE_SHOT\n",
    "        self.metrics = ['accuracy']\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        if self.mlp_one_shot:\n",
    "            self.weights_file = 'LOGS/shared_weights.pkl'\n",
    "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
    "            if not os.path.exists(self.weights_file):\n",
    "                print(\"Initializing shared weights dictionary...\")\n",
    "                self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def create_model(self, sequence, mlp_input_shape):\n",
    "        layer_configs = self.decode_sequence(sequence)\n",
    "        model = Sequential()\n",
    "\n",
    "        if len(mlp_input_shape) > 1:\n",
    "            model.add_module('flatten', Flatten(mlp_input_shape))\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if layer_conf == 'dropout':\n",
    "                    model.add_module('dropout', Dropout(self.mlp_dropout))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        else:\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if i == 0:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1], input_shape=mlp_input_shape))\n",
    "                elif layer_conf == 'dropout':\n",
    "                    model.add_module(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        if self.mlp_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.mlp_optimizer)(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay)\n",
    "\n",
    "        return optim\n",
    "\n",
    "    def update_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) == 0:\n",
    "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j], 'weights': layer.get_weights()}, ignore_index=True)\n",
    "                else:\n",
    "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def set_model_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                \n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) > 0:\n",
    "                    print(\"Transferring weights for layer:\", config_ids[j])\n",
    "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
    "                \n",
    "                j += 1\n",
    "\n",
    "    def train_model(self, model, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
    "        if self.mlp_one_shot:\n",
    "            self.set_model_weights(model)\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "            self.update_weights(model)\n",
    "        else:\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# generator = MLPGenerator()\n",
    "# model = generator.create_model(sequence, np.shape(x[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import LSTM, Linear\n",
    "\n",
    "# from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(MLPSearchSpace):\n",
    "\tdef __init__(self):\n",
    "\t\tself.max_len = MAX_ARCHITECTURE_LENGTH\n",
    "\t\tself.controller_lstm_dim = CONTROLLER_LSTM_DIM\n",
    "\t\tself.controller_optimizer = CONTROLLER_OPTIMIZER\n",
    "\t\tself.controller_lr = CONTROLLER_LEARNING_RATE\n",
    "\t\tself.controller_decay = CONTROLLER_DECAY\n",
    "\t\tself.controller_momentum = CONTROLLER_MOMENTUM\n",
    "\t\tself.use_predictor = CONTROLLER_USE_PREDICTOR\n",
    "\n",
    "\t\tself.controller_weights = 'LOGS/controller_weights.h5'\n",
    "\t\tself.seq_data = []\n",
    "\n",
    "\t\tsuper().__init__(TARGET_CLASSES)\n",
    "\n",
    "\t\tself.controller_classes = len(self.vocab) + 1\n",
    "        \n",
    "\t# Controller Architecture\n",
    "\tdef control_model(self, controller_input_shape, controller_batch_size):\n",
    "\t\tmain_input = torch.tensor(controller_batch_size, controller_input_shape) # name='main_input'\n",
    "\t\tx = LSTM(self.controller_lstm_dim)(main_input) # return_sequences=True\n",
    "\t\tmain_output = Linear(self.controller_classes)(x) # activation='softmax', name='main_output'\n",
    "\t\tmodel = Model(inputs=[main_input], outputs=[main_output]) #???????????\n",
    "\t\t\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef hybrid_control_model(self, controller_input_shape, controller_batch_size):\n",
    "\t\tmain_input = torch.tensor(controller_batch_size, controller_input_shape) # name='main_input'\n",
    "\t\tx = LSTM(self.controller_lstm_dim)(main_input) # return_sequences=True\n",
    "\t\tpredictor_output = Linear(1)(x) # , activation='sigmoid', name='predictor_output'\n",
    "\t\tmain_output = Linear(self.controller_classes)(x) # , activation='softmax', name='main_output'\n",
    "\t\tmodel = Model(inputs=[main_input], outputs=[main_output, predictor_output])\n",
    "\t\t\n",
    "\t\treturn model\n",
    "\t\n",
    "\tdef train_control_model(self, model, x_data, y_data, loss_func, controller_batch_size, nb_epochs):\n",
    "\t\tif self.controller_optimizer == 'sgd':\n",
    "\t\t\toptim = optimizers.SGD(lr=self.controller_lr, decay=self.controller_decay, momentum=self.controller_momentum, clipnorm=1.0)\n",
    "\t\telse:\n",
    "\t\t\toptim = getattr(optimizers, self.controller_optimizer)(lr=self.controller_lr, decay=self.controller_decay, clipnorm=1.0)\n",
    "\t\tmodel.compile(optimizer=optim, loss={'main_output': loss_func})\n",
    "\t\tif os.path.exists(self.controller_weights):\n",
    "\t\t\tmodel.load_weights(self.controller_weights)\n",
    "\t\tprint(\"TRAINING CONTROLLER...\")\n",
    "\t\tmodel.fit({'main_input': x_data},\n",
    "\t\t\t\t\t{'main_output': y_data.reshape(len(y_data), 1, self.controller_classes)},\n",
    "\t\t\t\t\tepochs=nb_epochs,\n",
    "\t\t\t\t\tbatch_size=controller_batch_size,\n",
    "\t\t\t\t\tverbose=0)\n",
    "\t\tmodel.save_weights(self.controller_weights)\n",
    "\n",
    "\tdef sample_architecture_sequences(self, model, number_of_samples):\n",
    "\t\tfinal_layer_id = len(self.vocab)\n",
    "\t\tdropout_id = final_layer_id - 1\n",
    "\t\tvocab_idx = [0] + list(self.vocab.keys())\n",
    "\t\tsamples = []\n",
    "\t\tprint(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
    "\t\tprint('------------------------------------------------------')\n",
    "\t\twhile len(samples) < number_of_samples:\n",
    "\t\t\tseed = []\n",
    "\t\t\twhile len(seed) < self.max_len:\n",
    "\t\t\t\tsequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
    "\t\t\t\tsequence = sequence.reshape(1, 1, self.max_len - 1)\n",
    "\t\t\t\tif self.use_predictor:\n",
    "\t\t\t\t\t(probab, _) = model.predict(sequence)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprobab = model.predict(sequence)\n",
    "\t\t\t\tprobab = probab[0][0]\n",
    "\t\t\t\tnext = np.random.choice(vocab_idx, size=1, p=probab)[0]\n",
    "\t\t\t\tif next == dropout_id and len(seed) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif next == final_layer_id and len(seed) == 0:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\tif next == final_layer_id:\n",
    "\t\t\t\t\tseed.append(next)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif len(seed) == self.max_len - 1:\n",
    "\t\t\t\t\tseed.append(final_layer_id)\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tif not next == 0:\n",
    "\t\t\t\t\tseed.append(next)\n",
    "\t\t\tif seed not in self.seq_data:\n",
    "\t\t\t\tsamples.append(seed)\n",
    "\t\t\t\tself.seq_data.append(seed)\n",
    "\t\treturn samples\n",
    "\t\n",
    "\tdef get_predicted_accuracies_hybrid_model(self, model, seqs):\n",
    "\t\tpred_accuracies = []\n",
    "\t\tfor seq in seqs:\n",
    "\t\t\tcontrol_sequences = pad_sequences([seq], maxlen=self.max_len, padding='post')\n",
    "\t\t\txc = control_sequences[:, :-1].reshape(len(control_sequences), 1, self.max_len - 1)\n",
    "\t\t\t(_, pred_accuracy) = [x[0][0] for x in model.predict(xc)]\n",
    "\t\t\tpred_accuracies.append(pred_accuracy[0])\n",
    "\t\treturn pred_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def clean_log():\n",
    "    filelist = os.listdir('LOGS')\n",
    "    for file in filelist:\n",
    "        if os.path.isfile('LOGS/{}'.format(file)):\n",
    "            os.remove('LOGS/{}'.format(file))\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def log_event():\n",
    "    dest = 'LOGS'\n",
    "    while os.path.exists(dest):\n",
    "        dest = 'LOGS/event{}'.format(np.random.randint(10000))\n",
    "    os.mkdir(dest)\n",
    "    filelist = os.listdir('LOGS')\n",
    "    for file in filelist:\n",
    "        if os.path.isfile('LOGS/{}'.format(file)):\n",
    "            shutil.move('LOGS/{}'.format(file),dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNAS(Controller):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.controller_sampling_epochs = CONTROLLER_SAMPLING_EPOCHS\n",
    "        self.samples_per_controller_epoch = SAMPLES_PER_CONTROLLER_EPOCH\n",
    "        self.controller_train_epochs = CONTROLLER_TRAINING_EPOCHS\n",
    "        self.architecture_train_epochs = ARCHITECTURE_TRAINING_EPOCHS\n",
    "        self.controller_loss_alpha = CONTROLLER_LOSS_ALPHA\n",
    "\n",
    "        self.data = []\n",
    "        self.nas_data_log = 'LOGS/nas_data.pkl'\n",
    "        clean_log()\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_generator = MLPGenerator()\n",
    "\n",
    "        self.controller_batch_size = len(self.data)\n",
    "        self.controller_input_shape = (1, MAX_ARCHITECTURE_LENGTH - 1)\n",
    "        print(self.controller_batch_size)\n",
    "        print(self.controller_input_shape)\n",
    "\n",
    "        if self.use_predictor:\n",
    "            self.controller_model = self.hybrid_control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "        else:\n",
    "            self.controller_model = self.control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "\n",
    "    # Training MLP models\n",
    "    def create_architecture(self, sequence):\n",
    "        if self.target_classes == 2:\n",
    "            self.model_generator.loss_func = 'binary_crossentropy'\n",
    "        model = self.model_generator.create_model(sequence, np.shape(self.x[0]))\n",
    "        model = self.model_generator.compile_model(model)\n",
    "        return model\n",
    "    \n",
    "    def train_architecture(self, model):\n",
    "        x, y = unison_shuffled_copies(self.x, self.y)\n",
    "        history = self.model_generator.train_model(model, x, y, self.architecture_train_epochs)\n",
    "        return history\n",
    "    \n",
    "    # Storing the training metrics\n",
    "    def append_model_metrics(self, sequence, history, pred_accuracy=None):\n",
    "        if len(history.history['val_accuracy']) == 1:\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                    history.history['val_accuracy'][0],\n",
    "                                    pred_accuracy])\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                    history.history['val_accuracy'][0]])\n",
    "            print('validation accuracy: ', history.history['val_accuracy'][0])\n",
    "        else:\n",
    "            val_acc = np.ma.average(history.history['val_accuracy'],\n",
    "                                    weights=np.arange(1, len(history.history['val_accuracy']) + 1),\n",
    "                                    axis=-1)\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                    val_acc,\n",
    "                                    pred_accuracy])\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                    val_acc])\n",
    "            print('validation accuracy: ', val_acc)\n",
    "    \n",
    "    # Preparing data for controller\n",
    "    def prepare_controller_data(self, sequences):\n",
    "        controller_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
    "        xc = controller_sequences[:, :-1].reshape(len(controller_sequences), 1, self.max_len - 1)\n",
    "        yc = to_categorical(controller_sequences[:, -1], self.controller_classes)\n",
    "        val_acc_target = [item[1] for item in self.data]\n",
    "        return xc, yc, val_acc_target\n",
    "    \n",
    "    # Implementing REINFORCE Gradient\n",
    "    def get_discounted_reward(self, rewards):\n",
    "        discounted_r = np.zeros_like(rewards, dtype=np.float32)\n",
    "        for t in range(len(rewards)):\n",
    "            running_add = 0.\n",
    "            exp = 0.\n",
    "            for r in rewards[t:]:\n",
    "                running_add += self.controller_loss_alpha**exp * r\n",
    "                exp += 1\n",
    "            discounted_r[t] = running_add\n",
    "        discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
    "        return discounted_r\n",
    "\n",
    "    def custom_loss(self, target, output):\n",
    "        baseline = 0.5\n",
    "        reward = np.array([item[1] - baseline for item in self.data[-self.samples_per_controller_epoch:]]).reshape(\n",
    "            self.samples_per_controller_epoch, 1)\n",
    "        discounted_reward = self.get_discounted_reward(reward)\n",
    "        loss = - K.log(output) * discounted_reward[:, None]\n",
    "        return loss\n",
    "    \n",
    "    # Training the controller\n",
    "    def train_controller(self, model, x, y, pred_accuracy=None):\n",
    "        if self.use_predictor:\n",
    "            self.train_hybrid_model(model,\n",
    "                                    x,\n",
    "                                    y,\n",
    "                                    pred_accuracy,\n",
    "                                    self.custom_loss,\n",
    "                                    len(self.data),\n",
    "                                    self.controller_train_epochs)\n",
    "        else:\n",
    "            self.train_control_model(model,\n",
    "                                     x,\n",
    "                                     y,\n",
    "                                     self.custom_loss,\n",
    "                                     len(self.data),\n",
    "                                     self.controller_train_epochs)\n",
    "\n",
    "    # The Main NAS loop\n",
    "    def search(self):\n",
    "        for controller_epoch in range(self.controller_sampling_epochs):\n",
    "            print('------------------------------------------------------------------')\n",
    "            print('                       CONTROLLER EPOCH: {}'.format(controller_epoch))\n",
    "            print('------------------------------------------------------------------')\n",
    "            sequences = self.sample_architecture_sequences(self.controller_model, self.samples_per_controller_epoch)\n",
    "            if self.use_predictor:\n",
    "                pred_accuracies = self.get_predicted_accuracies_hybrid_model(self.controller_model, sequences)\n",
    "            for i, sequence in enumerate(sequences):\n",
    "                print('Architecture: ', self.decode_sequence(sequence))\n",
    "                model = self.create_architecture(sequence)\n",
    "                history = self.train_architecture(model)\n",
    "                if self.use_predictor:\n",
    "                    self.append_model_metrics(sequence, history, pred_accuracies[i])\n",
    "                else:\n",
    "                    self.append_model_metrics(sequence, history)\n",
    "                print('------------------------------------------------------')\n",
    "            xc, yc, val_acc_target = self.prepare_controller_data(sequences)\n",
    "            self.train_controller(self.controller_model,\n",
    "                                    xc,\n",
    "                                    yc,\n",
    "                                    val_acc_target[-self.samples_per_controller_epoch:])\n",
    "        with open(self.nas_data_log, 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "        log_event()\n",
    "        \n",
    "        return self.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running MLPNAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def get_latest_event_id():\n",
    "    all_subdirs = ['LOGS/' + d for d in os.listdir('LOGS') if os.path.isdir('LOGS/' + d)]\n",
    "    latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
    "    return int(latest_subdir.replace('LOGS/event', ''))\n",
    "\n",
    "def load_nas_data():\n",
    "    event = get_latest_event_id()\n",
    "    data_file = 'LOGS/event{}/nas_data.pkl'.format(event)\n",
    "    with open(data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def sort_search_data(nas_data):\n",
    "    val_accs = [item[1] for item in nas_data]\n",
    "    sorted_idx = np.argsort(val_accs)[::-1]\n",
    "    nas_data = [nas_data[x] for x in sorted_idx]\n",
    "    return nas_data\n",
    "\n",
    "def get_top_n_architectures(n):\n",
    "    data = load_nas_data()\n",
    "    data = sort_search_data(data)\n",
    "    search_space = MLPSearchSpace(TARGET_CLASSES)\n",
    "    print('Top {} Architectures:'.format(n))\n",
    "    for seq_data in data[:n]:\n",
    "        print('Architecture', search_space.decode_sequence(seq_data[0]))\n",
    "        print('Validation Accuracy:', seq_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing shared weights dictionary...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AI\\Desktop\\NAS\\Tutorial\\Part2\\part2.ipynb 셀 20\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mget_dummies(data[\u001b[39m'\u001b[39m\u001b[39mquality_label\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mvalues\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# x : (4898, 13)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# y : (4898, 3)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m nas_object \u001b[39m=\u001b[39m MLPNAS(x, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m data \u001b[39m=\u001b[39m nas_object\u001b[39m.\u001b[39msearch()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m get_top_n_architectures(TOP_N)\n",
      "\u001b[1;32mc:\\Users\\AI\\Desktop\\NAS\\Tutorial\\Part2\\part2.ipynb 셀 20\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_input_shape \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, MAX_ARCHITECTURE_LENGTH \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_predictor:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhybrid_control_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontroller_input_shape, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontroller_batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_input_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_batch_size)\n",
      "\u001b[1;32mc:\\Users\\AI\\Desktop\\NAS\\Tutorial\\Part2\\part2.ipynb 셀 20\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhybrid_control_model\u001b[39m(\u001b[39mself\u001b[39m, controller_input_shape, controller_batch_size):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \tmain_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(controller_batch_size, controller_input_shape) \u001b[39m# name='main_input'\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \tx \u001b[39m=\u001b[39m LSTM(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontroller_lstm_dim)(main_input) \u001b[39m# return_sequences=True\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AI/Desktop/NAS/Tutorial/Part2/part2.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \tpredictor_output \u001b[39m=\u001b[39m Linear(\u001b[39m1\u001b[39m)(x) \u001b[39m# , activation='sigmoid', name='predictor_output'\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('DATASETS/wine-quality.csv')\n",
    "x = data.drop('quality_label', axis=1, inplace=False).values\n",
    "y = pd.get_dummies(data['quality_label']).values\n",
    "\n",
    "# x : (4898, 13)\n",
    "# y : (4898, 3)\n",
    "\n",
    "nas_object = MLPNAS(x, y)\n",
    "data = nas_object.search()\n",
    "\n",
    "get_top_n_architectures(TOP_N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
