{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSearchSpace(object):\n",
    "\n",
    "    def __init__(self, target_classes):\n",
    "\n",
    "        self.target_classes = target_classes\n",
    "        self.vocab = self.vocab_dict()\n",
    "\n",
    "\n",
    "    def vocab_dict(self):\n",
    "    \t# define the allowed nodes and activation functions\n",
    "        nodes = [8, 16, 32, 64, 128, 256, 512]\n",
    "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "        \n",
    "        # initialize lists for keys and values of the vocabulary\n",
    "        layer_params = []\n",
    "        layer_id = []\n",
    "        \n",
    "        # for all activation functions for each node\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(act_funcs)):\n",
    "            \t\n",
    "                # create an id and a configuration tuple (node, activation)\n",
    "                layer_params.append((nodes[i], act_funcs[j]))\n",
    "                layer_id.append(len(act_funcs) * i + j + 1)\n",
    "        \n",
    "        # zip the id and configurations into a dictionary\n",
    "        vocab = dict(zip(layer_id, layer_params))\n",
    "        \n",
    "        # add dropout in the volcabulary\n",
    "        vocab[len(vocab) + 1] = (('dropout'))\n",
    "        \n",
    "        # add the final softmax/sigmoid layer in the vocabulary\n",
    "        if self.target_classes == 2:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
    "        else:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
    "        return vocab\n",
    "\n",
    "\n",
    "\t# function to encode a sequence of configuration tuples\n",
    "    def encode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        encoded_sequence = []\n",
    "        for value in sequence:\n",
    "            encoded_sequence.append(keys[values.index(value)])\n",
    "        return encoded_sequence\n",
    "\n",
    "\n",
    "\t# function to decode a sequence back to configuration tuples\n",
    "    def decode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        decoded_sequence = []\n",
    "        for key in sequence:\n",
    "            decoded_sequence.append(values[keys.index(key)])\n",
    "        return decoded_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (8, 'sigmoid')\n",
      "2 (8, 'tanh')\n",
      "3 (8, 'relu')\n",
      "4 (8, 'elu')\n",
      "5 (16, 'sigmoid')\n",
      "6 (16, 'tanh')\n",
      "7 (16, 'relu')\n",
      "8 (16, 'elu')\n",
      "9 (32, 'sigmoid')\n",
      "10 (32, 'tanh')\n",
      "11 (32, 'relu')\n",
      "12 (32, 'elu')\n",
      "13 (64, 'sigmoid')\n",
      "14 (64, 'tanh')\n",
      "15 (64, 'relu')\n",
      "16 (64, 'elu')\n",
      "17 (128, 'sigmoid')\n",
      "18 (128, 'tanh')\n",
      "19 (128, 'relu')\n",
      "20 (128, 'elu')\n",
      "21 (256, 'sigmoid')\n",
      "22 (256, 'tanh')\n",
      "23 (256, 'relu')\n",
      "24 (256, 'elu')\n",
      "25 (512, 'sigmoid')\n",
      "26 (512, 'tanh')\n",
      "27 (512, 'relu')\n",
      "28 (512, 'elu')\n",
      "29 dropout\n",
      "30 (10, 'softmax')\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPSearchSpace(10)\n",
    "\n",
    "x = mlp.vocab_dict()\n",
    "for key, value in x.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   NAS PARAMETERS                     #\n",
    "########################################################\n",
    "CONTROLLER_SAMPLING_EPOCHS = 10\n",
    "SAMPLES_PER_CONTROLLER_EPOCH = 10\n",
    "CONTROLLER_TRAINING_EPOCHS = 10\n",
    "ARCHITECTURE_TRAINING_EPOCHS = 10\n",
    "CONTROLLER_LOSS_ALPHA = 0.9\n",
    "\n",
    "########################################################\n",
    "#               CONTROLLER PARAMETERS                  #\n",
    "########################################################\n",
    "CONTROLLER_LSTM_DIM = 100\n",
    "CONTROLLER_OPTIMIZER = 'Adam'\n",
    "CONTROLLER_LEARNING_RATE = 0.01\n",
    "CONTROLLER_DECAY = 0.1\n",
    "CONTROLLER_MOMENTUM = 0.0\n",
    "CONTROLLER_USE_PREDICTOR = True\n",
    "\n",
    "########################################################\n",
    "#                   MLP PARAMETERS                     #\n",
    "########################################################\n",
    "MAX_ARCHITECTURE_LENGTH = 3\n",
    "MLP_OPTIMIZER = 'Adam'\n",
    "MLP_LEARNING_RATE = 0.01\n",
    "MLP_DECAY = 0.0\n",
    "MLP_MOMENTUM = 0.0\n",
    "MLP_DROPOUT = 0.2\n",
    "MLP_LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "MLP_ONE_SHOT = True\n",
    "\n",
    "########################################################\n",
    "#                   DATA PARAMETERS                    #\n",
    "########################################################\n",
    "TARGET_CLASSES = 3\n",
    "\n",
    "########################################################\n",
    "#                  OUTPUT PARAMETERS                   #\n",
    "########################################################\n",
    "TOP_N = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim as optimizers\n",
    "from torch.nn import Sequential, Linear, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGenerator(MLPSearchSpace):\n",
    "    def __init__(self):\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.mlp_optimizer = MLP_OPTIMIZER\n",
    "        self.mlp_lr = MLP_LEARNING_RATE\n",
    "        self.mlp_decay = MLP_DECAY\n",
    "        self.mlp_momentum = MLP_MOMENTUM\n",
    "        self.mlp_dropout = MLP_DROPOUT\n",
    "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
    "        self.mlp_one_shot = MLP_ONE_SHOT\n",
    "        self.metrics = ['accuracy']\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        if self.mlp_one_shot:\n",
    "            self.weights_file = 'LOGS/shared_weights.pkl'\n",
    "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
    "            if not os.path.exists(self.weights_file):\n",
    "                print(\"Initializing shared weights dictionary...\")\n",
    "                self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def create_model(self, sequence, mlp_input_shape):\n",
    "        layer_configs = self.decode_sequence(sequence)\n",
    "        model = Sequential()\n",
    "        \n",
    "        if len(mlp_input_shape) > 1:\n",
    "            model.add_module('flatten', Flatten(mlp_input_shape))\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if layer_conf == 'dropout':\n",
    "                    model.add_module('dropout', Dropout(self.mlp_dropout))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1])) # 수정 필요\n",
    "        else:\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if i == 0:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1], input_shape=mlp_input_shape)) # 수정 필요\n",
    "                elif layer_conf == 'dropout':\n",
    "                    model.add_module('dropout', Dropout(self.mlp_dropout))\n",
    "                else:\n",
    "                    model.add_module('linear', Linear(units=layer_conf[0], activation=layer_conf[1])) # 수정 필요\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        if self.mlp_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.mlp_optimizer)(model.parameters(), lr=self.mlp_lr, weight_decay=self.mlp_decay)\n",
    "        \n",
    "        return optim\n",
    "\n",
    "    def update_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        \n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) == 0:\n",
    "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j], 'weights': layer.get_weights()}, ignore_index=True)\n",
    "                else:\n",
    "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
    "                \n",
    "                j += 1\n",
    "        \n",
    "        self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def set_model_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        \n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        \n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                \n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                \n",
    "                if len(search_index) > 0:\n",
    "                    print(\"Transferring weights for layer:\", config_ids[j])\n",
    "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
    "                \n",
    "                j += 1\n",
    "\n",
    "    def train_model(self, model, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
    "        if self.mlp_one_shot:\n",
    "            self.set_model_weights(model)\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "            self.update_weights(model)\n",
    "        else:\n",
    "            history = model.fit(x_data, y_data, epochs=nb_epochs, validation_split=validation_split, callbacks=callbacks, verbose=0)\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing shared weights dictionary...\n"
     ]
    }
   ],
   "source": [
    "a = MLPGenerator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
